{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Full credit goes to this blog post <a href='https://victorzhou.com/blog/intro-to-neural-networks/'>here</a> by Victor Zhou.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>This notebook creates a 3 input, 2 neuron hidden layer and 1 output, neural network from scratch.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    This Network takes in 3 inputs\n",
    "    Has 2 Hidden Neurons\n",
    "    And 1 Output Layer\n",
    "    All of the derivatives are written out as explicitly as possible\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "        self.w7 = np.random.normal()\n",
    "        self.w8 = np.random.normal()\n",
    "        \n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "        \n",
    "    def feed_forward(self,x):\n",
    "        z1 = self.w1 * x[:,0] + self.w2 * x[:,1] + self.w3 * x[:,2] + self.b1\n",
    "        h1 = sigmoid(z1)\n",
    "\n",
    "        z2 = self.w4 * x[:,0] + self.w5 * x[:,1] + self.w6 * x[:,2] + self.b2\n",
    "        h2 = sigmoid(z2)\n",
    "\n",
    "        z3 = self.w7*h1 + self.w8*h2 + self.b3\n",
    "        output = sigmoid(z3)\n",
    "        return output\n",
    "    \n",
    "    def train(self,data,all_y_trues,epochs,alpha):\n",
    "        for epoch in range(epochs):\n",
    "            for x,y_true in zip(data,all_y_trues):\n",
    "                #print(x,y_true)\n",
    "                z1 = self.w1 * x[0] + self.w2 * x[1] + self.w3 * x[2] + self.b1\n",
    "                h1 = sigmoid(z1)\n",
    "\n",
    "                z2 = self.w4 * x[0] + self.w5 * x[1] + self.w6 * x[2] + self.b2\n",
    "                h2 = sigmoid(z2)\n",
    "\n",
    "                z3 = self.w7*h1 + self.w8*h2 + self.b3\n",
    "                output = sigmoid(z3)\n",
    "                d_L_ypred = - 2 * (y_true - output)\n",
    "\n",
    "                #Output Neuron\n",
    "                d_ypred_d_z3 = sigmoid_derivative(z3)\n",
    "                d_z3_d_w7= h1\n",
    "                d_z3_d_w8 = h2\n",
    "                d_L_dw7 = d_L_ypred * d_ypred_d_z3 * d_z3_d_w7\n",
    "                d_L_dw8 = d_L_ypred * d_ypred_d_z3 * d_z3_d_w8\n",
    "                d_L_db3 = d_L_ypred * d_ypred_d_z3 \n",
    "\n",
    "                #Hidden Neuron 1\n",
    "                d_z3_d_h1 = self.w7\n",
    "                d_h1_d_z1 = sigmoid_derivative(z1)\n",
    "                d_z1_d_w1 = x[0]\n",
    "                d_z1_d_w2 = x[1]\n",
    "                d_z1_d_w3 = x[2]\n",
    "                d_L_dw1 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h1 * d_h1_d_z1 * d_z1_d_w1\n",
    "                d_L_dw2 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h1 * d_h1_d_z1 * d_z1_d_w2\n",
    "                d_L_dw3 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h1 * d_h1_d_z1 * d_z1_d_w3\n",
    "                d_L_db1 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h1 * d_h1_d_z1 \n",
    "\n",
    "                #Hidden Neuron 2\n",
    "                d_z3_d_h2 = self.w8\n",
    "                d_h2_d_z2 = sigmoid_derivative(z2)\n",
    "                d_z2_d_w4 = x[0]\n",
    "                d_z2_d_w5 = x[1]\n",
    "                d_z2_d_w6 = x[2]\n",
    "                d_L_dw4 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h2 * d_h2_d_z2 * d_z2_d_w4\n",
    "                d_L_dw5 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h2 * d_h2_d_z2 * d_z2_d_w5\n",
    "                d_L_dw6 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h2 * d_h2_d_z2 * d_z2_d_w6\n",
    "                d_L_db2 = d_L_ypred * d_ypred_d_z3 * d_z3_d_h2 * d_h2_d_z2\n",
    "                \n",
    "                self.w1 -= alpha * d_L_dw1\n",
    "                self.w2 -= alpha * d_L_dw2\n",
    "                self.w3 -= alpha * d_L_dw3\n",
    "                self.b1 -= alpha * d_L_db1\n",
    "\n",
    "                self.w4 -= alpha * d_L_dw4\n",
    "                self.w5 -= alpha * d_L_dw5\n",
    "                self.w6 -= alpha * d_L_dw6\n",
    "                self.b2 -= alpha * d_L_db2\n",
    "\n",
    "                self.w7 -= alpha * d_L_dw7\n",
    "                self.w8 -= alpha * d_L_dw8\n",
    "                self.b3 -= alpha * d_L_db3\n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = self.feed_forward(data)\n",
    "                loss = mse_loss(all_y_trues,y_pred)\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>We are going to use the Neural Network on this table of values</center></h3>\n",
    "\n",
    "|$X_{1}$|$X_{2}$|$X_{3}$|Y|\n",
    "|-|-|-|-|\n",
    "|1|0|0|0|\n",
    "|1|1|0|1|\n",
    "|1|0|1|1|\n",
    "|1|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.439\n",
      "Epoch 100 loss: 0.249\n",
      "Epoch 200 loss: 0.246\n",
      "Epoch 300 loss: 0.244\n",
      "Epoch 400 loss: 0.241\n",
      "Epoch 500 loss: 0.237\n",
      "Epoch 600 loss: 0.231\n",
      "Epoch 700 loss: 0.224\n",
      "Epoch 800 loss: 0.217\n",
      "Epoch 900 loss: 0.208\n",
      "Epoch 1000 loss: 0.199\n",
      "Epoch 1100 loss: 0.188\n",
      "Epoch 1200 loss: 0.171\n",
      "Epoch 1300 loss: 0.146\n",
      "Epoch 1400 loss: 0.118\n",
      "Epoch 1500 loss: 0.091\n",
      "Epoch 1600 loss: 0.070\n",
      "Epoch 1700 loss: 0.056\n",
      "Epoch 1800 loss: 0.045\n",
      "Epoch 1900 loss: 0.037\n",
      "Epoch 2000 loss: 0.031\n",
      "Epoch 2100 loss: 0.027\n",
      "Epoch 2200 loss: 0.024\n",
      "Epoch 2300 loss: 0.021\n",
      "Epoch 2400 loss: 0.019\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1,0,0],\n",
    "    [1,1,0],\n",
    "    [1,0,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "Y = np.array([0,1,1,0])\n",
    "NN = NeuralNetwork()\n",
    "NN.train(X,Y,2500,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13964953, 0.88084825, 0.87978237, 0.13729175])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.feed_forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>It goes a pretty good job for a bare minimun Neural Network, with more iterations or a different value of alpha you could achieve even better results.</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
